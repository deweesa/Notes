## Data Intensive Applications
Applications are considered "Data Intensive" if data is biggest challenge (compared to something like compute time).  Common challenges for data intensive apps include:
- Database storage
- Caching expensive data retrievals
- Indexing for search/filtering
- [Stream Processing]
- [Batch Processing]
There are three main concerns for these systems:
1. Reliability
2. Scalability
3. Maintainability
### Reliability
A system is "reliable" if things work correctly even when things go wrong, like faults within a system. Faults are not necessarily failures, as there are different types of faults some of which are unavoidable. It's more important to make a system fault tolerant instead of trying to prevent all faults
#### Hardware Faults
Sometimes hardware fails. This could be a single node coming down due to something like a hard-drive or some other component failing or the power going out. This kind of fault will happen, and as the number of machines are scaled up so are the expected faults. Tolerant systems will handle nodes going down, but this of course adds complexity
#### Software Errors
Software errors tend to be more systematic and can be harder to identify than hardware faults. This could be some bug that wasn't caught in testing, or some assumption that was made some time ago that no longer holds true. These can be prevented through thorough testing land good decision making.
#### Human Errors
Humans write software and manage all these systems. They can introduce errors either through bad acting, poor management systems, or even just simple mistakes. While poor management is hard to guard against there should be some level of prevention in terms of privileged access and handling faulty input to a system. 
### Scalability
Common reasons for a system to degrade is not handling increased load, or sudden influx of load. In order to determine scalability we need to be able to describe load through **Load Parameters**. Are we worried about reads/second or writes/second? And after we determine our load parameters we can describe performance. Common metric for performance is response time or throughput, and looking at the median performance. Companies like Amazon might be concerned with metrics like the 99.9th percentile response time, as they determined that increased response time reduces sales. 

There are two main strategies for coping with load. [Horizontal Scaling] and [Vertical Scaling]. Horizontal scaling refers to spreading load to other machines in some sort of distributed system, but this adds complexity. Vertical scaling refers to making a single node more powerful (i.e. adding more RAM or a bigger CPU) which is simpler but is more limited. There are also some systems that are **elastic** and are able to scale on demand, which of course adds complexity as we add or remove nodes from the system. 
#### Maintainability
At some point the system being worked on will either need to be changed or worked on in some way. For systems to be maintainable we want to focus on the **Operability/Simplicity/Evolvability** of a system. 

For the operability of a system there needs to be some visibility of how the system is working. Are we able to monitor the system effectively? Are we dependent on a single node that could fail and bring the system down? Do we have good documentation and playbooks for when things go wrong?

Simplicity is exactly what it sounds like, is it reasonably simple to make changes to the system or understand what is going on within it? It becomes hard to work on a system if it's not understood what is going on within it. 

The evolvability of a system somewhat relies on it's simplicity. Are things abstracted in a meaningful way? Are we able to reuse some components when new features are added on or removed? We want change to be easy as initial requirements or considerations are likely to change. 